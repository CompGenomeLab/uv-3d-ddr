{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import bioframe\n",
    "import multiprocessing as mp\n",
    "import glob\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "interval_size = 1_000  # bigwigs will have this resolution as interval size\n",
    "\n",
    "chromsizes = bioframe.fetch_chromsizes(\"hg38\")[:23]\n",
    "NPROC = 18\n",
    "\n",
    "bins_df = []\n",
    "for chrom, size in chromsizes.items():\n",
    "    bins_df.append(\n",
    "        pd.DataFrame(\n",
    "            {\n",
    "                \"chrom\": chrom,\n",
    "                \"start\": np.arange(0, size // interval_size * interval_size, interval_size),\n",
    "                \"end\": np.arange(interval_size, (size + interval_size) // interval_size * interval_size, interval_size),\n",
    "                \"count\": 0,\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "\n",
    "bins_df = pd.concat(\n",
    "    bins_df\n",
    ")  # an empty dataframe with all the bins as coordinates, this will have the counts added to it\n",
    "\n",
    "bins_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "blacklist_path = \"/cta/users/vkaya/hi-c/work/hela/notebooks/repair-data/hg38_blacklist.bed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class xr_ds_exp:\n",
    "    def __init__(self, file_path, blacklist=None):\n",
    "        self.file_path = file_path\n",
    "        if \"sim\" in file_path:\n",
    "            self.sim = True\n",
    "        else:\n",
    "            self.sim = False\n",
    "\n",
    "        if \"64\" in file_path:\n",
    "            self.lesion_type = \"64\"\n",
    "        elif \"CPD\" in file_path:\n",
    "            self.lesion_type = \"CPD\"\n",
    "        else:\n",
    "            self.lesion_type = None\n",
    "\n",
    "        if \"plus\" in file_path:\n",
    "            self.strand = \"plus\"\n",
    "        elif \"minus\" in file_path:\n",
    "            self.strand = \"minus\"\n",
    "        else:\n",
    "            self.strand = None\n",
    "\n",
    "        if \"XR\" in file_path:\n",
    "            self.experiment = \"XR\"\n",
    "        elif \"DS\" in file_path:\n",
    "            self.experiment = \"DS\"\n",
    "        else:\n",
    "            self.experiment = None\n",
    "\n",
    "        if 'rep1' in file_path:\n",
    "            self.rep = 'rep1'\n",
    "        elif 'rep2' in file_path:\n",
    "            self.rep = 'rep2'\n",
    "        else:\n",
    "            self.rep = None\n",
    "\n",
    "        self.df = pd.read_csv(file_path, sep=\"\\t\", header=None)\n",
    "        self.df.columns = [\"chrom\", \"start\", \"end\", \"name\", \"read_length\", \"strand\"]\n",
    "\n",
    "        self.exact_sites = False\n",
    "\n",
    "        if blacklist is not None:\n",
    "            blacklist_df = bioframe.read_table(blacklist).iloc[:, :3]\n",
    "            blacklist_df.columns = ['chrom', 'start', 'end']\n",
    "            self.blacklist_df = blacklist_df\n",
    "            self.filter_blacklist()\n",
    "\n",
    "    def intersect(self, bins_df, chromNames, interval_size):\n",
    "        df = self.df.copy()\n",
    "        all_bins_df = []\n",
    "        for chrName, chrDF in (\n",
    "            df.loc[df.chrom.isin(chromNames)].groupby([\"chrom\"]).__iter__()\n",
    "        ):\n",
    "            bins_df_curr = (\n",
    "                bins_df.copy().loc[bins_df.chrom == chrName].reset_index(drop=True)\n",
    "            )\n",
    "            start_pos = chrDF.start\n",
    "            bins = start_pos // interval_size\n",
    "            counts = bins.groupby(bins).count()\n",
    "            bins_df_curr.loc[counts.index, \"count\"] = counts.values\n",
    "            all_bins_df.append(bins_df_curr)\n",
    "\n",
    "        self.df_intersect = pd.concat(all_bins_df)\n",
    "\n",
    "    def get_exact_sites(self):\n",
    "        if self.exact_sites == False:\n",
    "            if self.experiment == \"XR\" and self.strand == \"plus\":\n",
    "                self.df.start = self.df.end - 8\n",
    "                self.df.end = self.df.end - 6\n",
    "                self.exact_sites = True\n",
    "            elif self.experiment == \"XR\" and self.strand == \"minus\":\n",
    "                self.df.start = self.df.start + 6\n",
    "                self.df.end = self.df.start + 8\n",
    "                self.exact_sites = True\n",
    "            elif self.experiment == \"DS\" and self.strand == \"plus\":\n",
    "                self.df.start = self.df.end - 5\n",
    "                self.df.end = self.df.end - 3\n",
    "                self.exact_sites = True\n",
    "            elif self.experiment == \"DS\" and self.strand == \"minus\":\n",
    "                self.df.start = self.df.start + 3\n",
    "                self.df.end = self.df.start + 5\n",
    "                self.exact_sites = True\n",
    "            self.df.sort_values(by=[\"chrom\", \"start\", \"end\"], inplace=True)\n",
    "        else:\n",
    "            print(f\"exact_sites {self.exact_sites}\")\n",
    "\n",
    "    def filter_blacklist(self):\n",
    "        if self.blacklist_df is not None:\n",
    "            self.df = bioframe.ops.subtract(self.df, self.blacklist_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersect_wrapper(file_in, bins_df, chromNames, blacklist_path, interval_size):\n",
    "    exp = xr_ds_exp(file_in, blacklist_path)\n",
    "    exp.get_exact_sites()\n",
    "    exp.intersect(bins_df, chromNames, interval_size)\n",
    "    return exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "chromNames = chromsizes.keys()\n",
    "path = \"/cta/users/vkaya/rep_dmg_snakemake/dedup/xr-ds-seq-snakemake/results/processed_files/*.bed\"\n",
    "args = [(file_in, bins_df, chromNames, blacklist_path, interval_size) for file_in in glob.glob(path)]\n",
    "with mp.Pool(NPROC) as pool:\n",
    "    results = pool.starmap(intersect_wrapper, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_name(exp):\n",
    "    return (\n",
    "        f\"{exp.experiment}_{exp.lesion_type}_{exp.strand}_{exp.rep}_sim\"\n",
    "        if exp.sim\n",
    "        else f\"{exp.experiment}_{exp.lesion_type}_{exp.strand}_{exp.rep}_real\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing XR_CPD_minus_rep1_sim and XR_CPD_plus_rep1_sim\n",
      "Key name: XR_CPD_rep1_sim\n",
      "Processing DS_CPD_plus_rep1_real and DS_CPD_minus_rep1_real\n",
      "Key name: DS_CPD_rep1_real\n",
      "Processing XR_CPD_minus_rep1_real and XR_CPD_plus_rep1_real\n",
      "Key name: XR_CPD_rep1_real\n",
      "Processing XR_64_minus_rep2_sim and XR_64_plus_rep2_sim\n",
      "Key name: XR_64_rep2_sim\n",
      "Processing DS_CPD_minus_rep1_sim and DS_CPD_plus_rep1_sim\n",
      "Key name: DS_CPD_rep1_sim\n",
      "Processing DS_CPD_plus_rep2_real and DS_CPD_minus_rep2_real\n",
      "Key name: DS_CPD_rep2_real\n",
      "Processing XR_64_minus_rep1_real and XR_64_plus_rep1_real\n",
      "Key name: XR_64_rep1_real\n",
      "Processing DS_64_plus_rep2_real and DS_64_minus_rep2_real\n",
      "Key name: DS_64_rep2_real\n",
      "Processing DS_64_plus_rep1_sim and DS_64_minus_rep1_sim\n",
      "Key name: DS_64_rep1_sim\n",
      "Processing XR_64_plus_rep1_sim and XR_64_minus_rep1_sim\n",
      "Key name: XR_64_rep1_sim\n",
      "Processing DS_64_plus_rep1_real and DS_64_minus_rep1_real\n",
      "Key name: DS_64_rep1_real\n",
      "Processing DS_64_plus_rep2_sim and DS_64_minus_rep2_sim\n",
      "Key name: DS_64_rep2_sim\n",
      "Processing XR_CPD_minus_rep2_sim and XR_CPD_plus_rep2_sim\n",
      "Key name: XR_CPD_rep2_sim\n",
      "Processing XR_CPD_minus_rep2_real and XR_CPD_plus_rep2_real\n",
      "Key name: XR_CPD_rep2_real\n",
      "Processing XR_64_plus_rep2_real and XR_64_minus_rep2_real\n",
      "Key name: XR_64_rep2_real\n",
      "Processing DS_CPD_minus_rep2_sim and DS_CPD_plus_rep2_sim\n",
      "Key name: DS_CPD_rep2_sim\n"
     ]
    }
   ],
   "source": [
    "## merge plus and minus strands\n",
    "\n",
    "plus_minus_merged = {}\n",
    "\n",
    "prev = []\n",
    "for res_1 in results:\n",
    "    for res_2 in results:\n",
    "        name_1 = get_name(res_1)\n",
    "        name_2 = get_name(res_2)\n",
    "\n",
    "        if set((name_1, name_2)) in prev:\n",
    "            continue\n",
    "        elif name_1 == name_2:\n",
    "            continue\n",
    "        else:\n",
    "            prev.append(set((name_1, name_2)))\n",
    "\n",
    "            if (\n",
    "                res_1.experiment == res_2.experiment\n",
    "                and res_1.lesion_type == res_2.lesion_type\n",
    "                and res_1.sim == res_2.sim\n",
    "                and res_1.rep == res_2.rep\n",
    "                and res_1.strand != res_2.strand\n",
    "            ):\n",
    "                key_name = (\n",
    "                    f\"{res_1.experiment}_{res_1.lesion_type}_{res_1.rep}_sim\"\n",
    "                    if res_1.sim\n",
    "                    else f\"{res_1.experiment}_{res_1.lesion_type}_{res_1.rep}_real\"\n",
    "                )\n",
    "                print(f\"Processing {name_1} and {name_2}\")\n",
    "                print(f\"Key name: {key_name}\")\n",
    "                intersect_1 = res_1.df_intersect.copy()\n",
    "                intersect_2 = res_2.df_intersect.copy()\n",
    "                assert (\n",
    "                    intersect_1[\"chrom\"].values.tolist()\n",
    "                    == intersect_2[\"chrom\"].values.tolist()\n",
    "                )\n",
    "                counts_1 = intersect_1[\"count\"].values\n",
    "                counts_2 = intersect_2[\"count\"].values\n",
    "                counts = counts_1 + counts_2\n",
    "                intersect_1[\"count\"] = counts\n",
    "\n",
    "                plus_minus_merged[key_name] = intersect_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing XR_CPD_rep1_sim and XR_CPD_rep2_sim\n",
      "Key name: XR_CPD_sim\n",
      "Processing DS_CPD_rep1_real and DS_CPD_rep2_real\n",
      "Key name: DS_CPD_real\n",
      "Processing XR_CPD_rep1_real and XR_CPD_rep2_real\n",
      "Key name: XR_CPD_real\n",
      "Processing XR_64_rep2_sim and XR_64_rep1_sim\n",
      "Key name: XR_64_sim\n",
      "Processing DS_CPD_rep1_sim and DS_CPD_rep2_sim\n",
      "Key name: DS_CPD_sim\n",
      "Processing XR_64_rep1_real and XR_64_rep2_real\n",
      "Key name: XR_64_real\n",
      "Processing DS_64_rep2_real and DS_64_rep1_real\n",
      "Key name: DS_64_real\n",
      "Processing DS_64_rep1_sim and DS_64_rep2_sim\n",
      "Key name: DS_64_sim\n"
     ]
    }
   ],
   "source": [
    "## merge rep1 and rep2\n",
    "\n",
    "rep1_rep2_merged = {}\n",
    "\n",
    "prev = []\n",
    "for k_1 in plus_minus_merged:\n",
    "    for k_2 in plus_minus_merged:\n",
    "        if set((k_1, k_2)) in prev:\n",
    "            continue\n",
    "        elif k_1 == k_2:\n",
    "            continue\n",
    "        else:\n",
    "            prev.append(set((k_1, k_2)))\n",
    "\n",
    "            if (\n",
    "                k_1.split(\"_\")[0] == k_2.split(\"_\")[0]\n",
    "                and k_1.split(\"_\")[1] == k_2.split(\"_\")[1]\n",
    "                and k_1.split(\"_\")[2] != k_2.split(\"_\")[2]\n",
    "                and k_1.split(\"_\")[3] == k_2.split(\"_\")[3]\n",
    "            ):\n",
    "                key_name = \"_\".join([l for i,l in enumerate(k_1.split(\"_\")) if i != 2])\n",
    "                print(f\"Processing {k_1} and {k_2}\")\n",
    "                print(f\"Key name: {key_name}\")\n",
    "                intersect_1 = plus_minus_merged[k_1].copy()\n",
    "                intersect_2 = plus_minus_merged[k_2].copy()\n",
    "                assert (\n",
    "                    intersect_1[\"chrom\"].values.tolist()\n",
    "                    == intersect_2[\"chrom\"].values.tolist()\n",
    "                )\n",
    "                counts_1 = intersect_1[\"count\"].values\n",
    "                counts_2 = intersect_2[\"count\"].values\n",
    "                counts = counts_1 + counts_2\n",
    "                intersect_1[\"count\"] = counts\n",
    "                rep1_rep2_merged[key_name] = intersect_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing XR_CPD_sim\n",
      "Total counts: 26116696\n",
      "Normalizing DS_CPD_real\n",
      "Total counts: 16822127\n",
      "Normalizing XR_CPD_real\n",
      "Total counts: 26452698\n",
      "Normalizing XR_64_sim\n",
      "Total counts: 35161521\n",
      "Normalizing DS_CPD_sim\n",
      "Total counts: 15556897\n",
      "Normalizing XR_64_real\n",
      "Total counts: 35619658\n",
      "Normalizing DS_64_real\n",
      "Total counts: 18575746\n",
      "Normalizing DS_64_sim\n",
      "Total counts: 17106264\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import gmean\n",
    "def normalize(counts_df, add_pseudocount=None):\n",
    "    counts = counts_df[\"count\"]\n",
    "    print(f\"Total counts: {counts.sum()}\")\n",
    "    if add_pseudocount != None:\n",
    "        if type(add_pseudocount) == float:\n",
    "            counts += add_pseudocount\n",
    "            print(f\"Added {add_pseudocount} pseudocount\")\n",
    "        elif add_pseudocount == \"mean\":\n",
    "            pseudo_count = counts.mean()\n",
    "            counts += pseudo_count\n",
    "            print(f\"Added {pseudo_count} pseudocount\")\n",
    "        elif add_pseudocount == \"base_level\":\n",
    "            pseudo_count = counts.sum() / len(counts.loc[counts > 0])\n",
    "            counts += pseudo_count\n",
    "            print(f\"Added {pseudo_count} pseudocount\")\n",
    "    counts_df[\"count\"] = counts * 1_000_000 / counts.sum()\n",
    "    return counts_df\n",
    "\n",
    "normalized = {}\n",
    "for k, v in rep1_rep2_merged.items():\n",
    "    print(f\"Normalizing {k}\")\n",
    "    normalized[k] = normalize(v.copy(), add_pseudocount = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XR_CPD_sim 0.3299208519876079 0.3063174606772618 0.2000516621851465 999999.9999999992 0.0 4.2118650843123495\n",
      "DS_CPD_real 0.3299208519876081 0.2972275741349474 0.23895905999575145 999999.9999999998 0.0 4.696195671332169\n",
      "XR_CPD_real 0.32992085198760834 0.2268199636951966 0.3764337262729941 1000000.0000000005 0.0 11.681228130302625\n",
      "XR_64_sim 0.3299208519876081 0.31284198428162424 0.21079707674506215 999999.9999999999 0.0 3.7825439917687294\n",
      "DS_CPD_sim 0.3299208519876083 0.32140085519625156 0.20655582040402193 1000000.0000000003 0.0 41.846391346551954\n",
      "XR_64_real 0.3299208519876082 0.30881823739015124 0.25516259424220267 1000000.0 0.0 4.688422331286842\n",
      "DS_64_real 0.3299208519876083 0.3230018326047309 0.21848100059282238 1000000.0000000003 0.0 5.060362044140785\n",
      "DS_64_sim 0.32992085198760807 0.35074870819250775 0.17696543035162177 999999.9999999997 0.0 5.319688740919701\n"
     ]
    }
   ],
   "source": [
    "for k, v in normalized.items():\n",
    "    print(\n",
    "        k,\n",
    "        v[\"count\"].mean(),\n",
    "        v[\"count\"].median(),\n",
    "        v[\"count\"].std(),\n",
    "        v[\"count\"].sum(),\n",
    "        v[\"count\"].min(),\n",
    "        v[\"count\"].max(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_std(df, n_std=3):\n",
    "    arr = df['count'].copy()\n",
    "    arr = np.clip(arr, -n_std * np.nanstd(arr) + np.mean(arr), n_std * np.nanstd(arr) + np.mean(arr))\n",
    "    df['count'] = np.where(np.isnan(arr), np.nan, arr)\n",
    "    return df\n",
    "\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "def qt(df):\n",
    "    arr = df['count'].copy()\n",
    "    arr = arr.replace([np.inf, -np.inf], np.nan)\n",
    "    arr = np.where(np.isnan(arr), np.nan, arr)\n",
    "    arr = arr.reshape(-1, 1)\n",
    "    qt = QuantileTransformer(n_quantiles=1000, output_distribution='uniform')\n",
    "    arr = qt.fit_transform(arr)\n",
    "    df['count'] = arr * 2\n",
    "    return df\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "def zscore(df):\n",
    "    arr = df['count'].copy()\n",
    "    arr = arr.replace([np.inf, -np.inf], np.nan)\n",
    "    arr = np.where(np.isnan(arr), np.nan, arr)\n",
    "    arr = arr.reshape(-1, 1)\n",
    "    scaler = StandardScaler()\n",
    "    arr = scaler.fit_transform(arr)\n",
    "    df['count'] = arr\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "over_sim = {}\n",
    "\n",
    "over_sim[\"XR_CPD_real_over_sim\"] = normalized[\"XR_CPD_real\"].copy()\n",
    "over_sim[\"XR_CPD_real_over_sim\"][\"count\"] /= normalized[\"XR_CPD_sim\"][\"count\"]\n",
    "\n",
    "over_sim[\"XR_64_real_over_sim\"] = normalized[\"XR_64_real\"].copy()\n",
    "over_sim[\"XR_64_real_over_sim\"][\"count\"] /= normalized[\"XR_64_sim\"][\"count\"]\n",
    "\n",
    "over_sim[\"DS_CPD_real_over_sim\"] = normalized[\"DS_CPD_real\"].copy()\n",
    "over_sim[\"DS_CPD_real_over_sim\"][\"count\"] /= normalized[\"DS_CPD_sim\"][\"count\"]\n",
    "\n",
    "over_sim[\"DS_64_real_over_sim\"] = normalized[\"DS_64_real\"].copy()\n",
    "over_sim[\"DS_64_real_over_sim\"][\"count\"] /= normalized[\"DS_64_sim\"][\"count\"]\n",
    "\n",
    "over_sim[\"XR_CPD_rep_eff\"] = over_sim[\"XR_CPD_real_over_sim\"].copy()\n",
    "over_sim[\"XR_CPD_rep_eff\"][\"count\"] /= over_sim[\"DS_CPD_real_over_sim\"][\"count\"]\n",
    "\n",
    "over_sim[\"XR_64_rep_eff\"] = over_sim[\"XR_64_real_over_sim\"].copy()\n",
    "over_sim[\"XR_64_rep_eff\"][\"count\"] /= over_sim[\"DS_64_real_over_sim\"][\"count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XR_CPD_real_over_sim 1.182501818395577 0.7678984494427668 1.6456957689855902 3241411.311989581 0.0 100.70439665549428\n",
      "[  0.           0.37973      0.76789845   1.41042572 100.70439666]\n",
      "XR_64_real_over_sim 1.1320465635344819 0.9562900202115923 0.952388127834937 3112935.601804024 0.0 60.2154232081622\n",
      "[ 0.          0.59228285  0.95629002  1.419011   60.21542321]\n",
      "DS_CPD_real_over_sim 1.2889167795760257 0.9247877512754481 1.2824212858323185 3577641.149402056 0.0 36.5291161753802\n",
      "[ 0.          0.55487265  0.92478775  1.61837856 36.52911618]\n",
      "DS_64_real_over_sim 1.2508587221100007 0.9208924368367225 1.1746015595479498 3497988.8906189534 0.0 29.468557978775117\n",
      "[ 0.          0.57555777  0.92089244  1.53482073 29.46855798]\n",
      "XR_CPD_rep_eff 1.5661950273544312 0.7371483515021189 3.356393665850783 4133003.866175116 0.0 617.221940385326\n",
      "[0.00000000e+00 3.11381631e-01 7.37148352e-01 1.64714528e+00\n",
      " 6.17221940e+02]\n",
      "XR_64_rep_eff 1.5001647657629529 0.9095217587887839 2.320224109814775 3992888.0459919455 0.0 247.61729883024648\n",
      "[  0.           0.47641616   0.90952176   1.71509817 247.61729883]\n"
     ]
    }
   ],
   "source": [
    "for k, v in over_sim.items():\n",
    "    #over_sim[k] = qt(clip_std(v.reset_index(drop=True).replace([np.inf, -np.inf], np.nan)))\n",
    "    #over_sim[k] = clip_std(v.reset_index(drop=True).replace([np.inf, -np.inf], np.nan), 3)\n",
    "    over_sim[k] = v.reset_index(drop=True).replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "\n",
    "\n",
    "for k, v in over_sim.items():\n",
    "    print(\n",
    "        k,\n",
    "        v[\"count\"].mean(),\n",
    "        v[\"count\"].median(),\n",
    "        v[\"count\"].std(),\n",
    "        v[\"count\"].sum(),\n",
    "        v[\"count\"].min(),\n",
    "        v[\"count\"].max(),\n",
    "    )\n",
    "    a, b = pd.qcut(v[\"count\"], 4, retbins=True)\n",
    "    print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = \"/cta/users/vkaya/hi-c/work/hela/notebooks/repair-data/bws_bioframe\"\n",
    "\n",
    "with mp.Pool(8) as pool:\n",
    "    pool.starmap(\n",
    "        bioframe.to_bigwig,\n",
    "        [\n",
    "            (v, chromsizes, f\"{out_path}/{k}_res{interval_size}.bw\", \"count\")\n",
    "            for k, v in over_sim.items()\n",
    "        ],\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
